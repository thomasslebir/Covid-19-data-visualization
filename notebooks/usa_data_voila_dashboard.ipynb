{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.renderers.default = \"iframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nytimes_state_data(url=None):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of state-level Covid-19 data from the New York Times' GitHub.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to csv file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        url = r\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\"\n",
    "    \n",
    "    # Read data & pad FIPS\n",
    "    df = pd.read_csv(url)\n",
    "    df[\"fips\"] = df[\"fips\"].astype(int).astype(str).str.zfill(2)\n",
    "    df.rename(columns={\"cases\": \"cum_cases\", \"deaths\": \"cum_deaths\"}, inplace=True)\n",
    "    \n",
    "    # Make tidy\n",
    "    all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max()).astype(str).tolist()\n",
    "    frames = []\n",
    "    for state in df[\"state\"].unique():\n",
    "        df_state = df[df[\"state\"] == state].copy()\n",
    "        dates_to_add = list(set(all_dates).difference(set(df_state[\"date\"])))\n",
    "        df_to_add = pd.DataFrame({\"date\": dates_to_add})\n",
    "        df_to_add[\"state\"] = state\n",
    "        df_to_add[\"fips\"] = df_state[\"fips\"].iloc[0]\n",
    "        # Aggregate\n",
    "        df_state_new = pd.concat([df_state, df_to_add])\n",
    "        df_state_new.sort_values(by=\"date\", inplace=True)\n",
    "        for col in [\"cases\", \"deaths\"]:\n",
    "            df_state_new[\"cum_{}\".format(col)].fillna(method=\"ffill\", inplace=True)\n",
    "            df_state_new[\"cum_{}\".format(col)].fillna(0, inplace=True)\n",
    "            df_state_new[col] = df_state_new[\"cum_{}\".format(col)].diff().fillna(0)\n",
    "        frames.append(df_state_new)\n",
    "        \n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df.sort_values(by=[\"state\", \"date\"], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nytimes_county_data(url=None):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of county-level Covid-19 data from the New York Times' GitHub.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to csv file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        url = r\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv\"\n",
    "    \n",
    "    # Fill in missing FIPS & remove cases in unknown counties\n",
    "    df = pd.read_csv(url)\n",
    "    df.loc[df[\"county\"] == \"New York City\", \"fips\"] = 36061\n",
    "    df.loc[df[\"county\"] == \"Kansas City\", \"fips\"] = 20085\n",
    "    df = df[df[\"county\"] != \"Unknown\"].copy()\n",
    "    df[\"fips\"] = df[\"fips\"].astype(int).astype(str).str.zfill(5)\n",
    "    df.rename(columns={\"cases\": \"cum_cases\", \"deaths\": \"cum_deaths\"}, inplace=True)\n",
    "    \n",
    "    # Make tidy\n",
    "    all_dates = pd.date_range(df[\"date\"].min(), df[\"date\"].max()).astype(str).tolist()\n",
    "    frames = []\n",
    "    for fips in df[\"fips\"].unique():\n",
    "        df_county = df[df[\"fips\"] == fips].copy()\n",
    "        dates_to_add = list(set(all_dates).difference(set(df_county[\"date\"])))\n",
    "        df_to_add = pd.DataFrame({\"date\": dates_to_add})\n",
    "        df_to_add[\"county\"] = df_county[\"county\"].iloc[0]\n",
    "        df_to_add[\"state\"] = df_county[\"state\"].iloc[0]\n",
    "        df_to_add[\"fips\"] = fips\n",
    "        # Aggregate\n",
    "        df_county_new = pd.concat([df_county, df_to_add])\n",
    "        df_county_new.sort_values(by=\"date\", inplace=True)\n",
    "        for col in [\"cases\", \"deaths\"]:\n",
    "            df_county_new[\"cum_{}\".format(col)].fillna(method=\"ffill\", inplace=True)\n",
    "            df_county_new[\"cum_{}\".format(col)].fillna(0, inplace=True)\n",
    "            df_county_new[col] = df_county_new[\"cum_{}\".format(col)].diff().fillna(0)\n",
    "        frames.append(df_county_new)\n",
    "            \n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df.sort_values(by=[\"county\", \"date\"], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usa_state_codes(url=None, table_index=0):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of United States alpha and FIPS codes.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to webpage.\n",
    "        table_index (int): Position of the table of interest within the page's HTML elements.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        url = r\"https://www.nrcs.usda.gov/wps/portal/nrcs/detail/?cid=nrcs143_013696\"\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    df = tables[0][:-1].copy()\n",
    "    df.columns = [c.strip().lower().replace(\" \", \"_\") for c in df.columns]\n",
    "    df.rename(columns={\"postal_code\": \"alpha_code\"}, inplace=True)\n",
    "    df[\"fips\"] = df[\"fips\"].astype(int).astype(str).str.zfill(2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usa_state_population(url=None, skiprows=3, skipfooter=7, cols_to_keep=None):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of United States 2019 population estimates by state from the US Census website.\n",
    "    \n",
    "    Args:\n",
    "        url (str): Link to the US Census' Excel file.\n",
    "        skiprows (int): Rows to skip at the top of the file.\n",
    "        skipfooter (int): Rows to skip at the end of the file.\n",
    "        cols_to_keep (list): 0-indexed list of column indices to keep.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        url = r\"https://www2.census.gov/programs-surveys/popest/tables/2010-2019/state/totals/nst-est2019-01.xlsx\"\n",
    "        \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [0, -1]\n",
    "        \n",
    "    df = pd.read_excel(url, skiprows=skiprows, skipfooter=skipfooter)\n",
    "    df = df.iloc[:, cols_to_keep].copy()\n",
    "    df.columns = [\"state\", \"population\"]\n",
    "    df = df[df[\"state\"].str.startswith(\".\")].copy()\n",
    "    df[\"state\"] = df[\"state\"].str[1:]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usa_county_codes(url=None, table_index=0):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of United States FIPS county codes from the NRCS website.\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL to NRCS county FIPS codes page.\n",
    "        table_index (int): Position of the table of interest within the page's HTML elements.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        url = r\"https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697\"\n",
    "    tables = pd.read_html(url)\n",
    "    \n",
    "    df = tables[0][:-1].copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    df[\"fips\"] = df[\"fips\"].astype(int).astype(str).str.zfill(5)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_usa_county_population(url=None, skiprows=3, skipfooter=6, cols_to_keep=None):\n",
    "    \"\"\"\n",
    "    TBC\n",
    "    \"\"\"\n",
    "    if url is None:\n",
    "        url = r\"https://www2.census.gov/programs-surveys/popest/tables/2010-2019/counties/totals/co-est2019-annres.xlsx\"\n",
    "        \n",
    "    if cols_to_keep is None:\n",
    "        cols_to_keep = [0, -1]\n",
    "        \n",
    "    df = pd.read_excel(url, skiprows=skiprows, skipfooter=skipfooter)\n",
    "    df = df.iloc[:, cols_to_keep].copy()\n",
    "    df.columns = [\"county\", \"population\"]\n",
    "    df = df[df[\"county\"].str.startswith(\".\")].copy()\n",
    "    df[\"county\"] = df[\"county\"].str[1:]\n",
    "    df[\"county_state\"] = df[\"county\"].str.replace(\" County, \", \" \")\n",
    "    df[\"state\"] = df[\"county_state\"].str.split(\" \").str[-1]\n",
    "    df[\"county\"] = df[\"county_state\"].str.split(\" \").str[0]\n",
    "    df = df[[\"county_state\", \"state\", \"county\", \"population\"]]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state_codes = get_usa_state_codes()\n",
    "df_county_codes = get_usa_county_codes()\n",
    "df_states = get_nytimes_state_data()\n",
    "df_states = df_states.merge(df_state_codes[[\"fips\", \"alpha_code\"]], how=\"left\", left_on=\"fips\", right_on=\"fips\")\n",
    "df_counties = get_nytimes_county_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_url = r\"https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json\"\n",
    "res = requests.get(geojson_url)\n",
    "geojson = json.loads(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasslebir/opt/anaconda3/envs/covid_viz_env/lib/python3.8/site-packages/pandas/core/series.py:679: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log10\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_11.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.choropleth(df_counties, geojson=geojson_url, locations=\"fips\", scope=\"usa\", color=np.log10(df_counties[\"cum_cases\"]), \n",
    "                    animation_frame=\"date\", range_color=[0, np.log10(df_counties[\"cum_cases\"]).max()], color_continuous_scale=px.colors.sequential.Reds, hover_name=\"county\", hover_data=[\"cum_cases\"])\n",
    "fig.update_layout(coloraxis_colorbar=dict(title=\"Cumulative cases\", thicknessmode=\"pixels\", thickness=25, lenmode=\"pixels\", len=397, yanchor=\"middle\", y=.5, ticks=\"outside\", \n",
    "                                          tickvals=list(range(math.ceil(np.log10(df_counties[\"cum_cases\"].max())))),\n",
    "                                          ticktext=[round(10 ** x, 0) for x in list(range(math.ceil(np.log10(df_counties[\"cum_cases\"].max()))))]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.choropleth(df_states, locations=\"alpha_code\", locationmode=\"USA-states\", scope=\"usa\", color=np.log10(df_states[\"cum_cases\"]), range_color=[0, np.log10(df_states[\"cum_cases\"]).max()], color_continuous_scale=px.colors.sequential.Reds,\n",
    "                    hover_name=\"state\", hover_data=[\"cum_cases\"], animation_frame=\"date\")\n",
    "fig.update_layout(coloraxis_colorbar=dict(title=\"Cumulative cases\", thicknessmode=\"pixels\", thickness=25, lenmode=\"pixels\", len=397, yanchor=\"middle\", y=.5, ticks=\"outside\", \n",
    "                                          tickvals=list(range(math.ceil(np.log10(df_states[\"cum_cases\"].max())))),\n",
    "                                          ticktext=[round(10 ** x, 0) for x in list(range(math.ceil(np.log10(df_states[\"cum_cases\"].max()))))]))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('covid_viz_env': conda)",
   "language": "python",
   "name": "python38264bitcovidvizenvconda8161c9317c4541db97624dae488151e8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
